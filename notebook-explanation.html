<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Jupyter Notebook Explanation - Smart Credit Risk System</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸ›¡ï¸</text></svg>">
    
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- Sidebar Navigation -->
    <nav class="sidebar" id="sidebar">
        <div class="sidebar-header">
            <h1>Smart Credit Risk System</h1>
            <p>ML Documentation</p>
        </div>
        <ul class="sidebar-nav">
            <li><a href="index.html">ğŸ  Home</a></li>
            <li><a href="backend.html">âš™ï¸ Backend Walkthrough</a></li>
            <li><a href="app-explanation.html">ğŸ“ app.py Explanation</a></li>
            <li><a href="notebook-explanation.html" class="active">ğŸ““ Notebook Explanation</a></li>
           
            <li><a href="train-model-explanation.html">ğŸ¤– train_model.py</a></li>
            <li><a href="shrink-model-explanation.html">ğŸ—œï¸ shrink_model.py</a></li>
            <li><a href="interface.html">ğŸ’» Interface & Deployment</a></li>
            <li><a href="tools.html">ğŸ› ï¸ Tools & Dataset</a></li>
            <li><a href="faq.html">â“ FAQ & Interview Q&A</a></li>
        </ul>
    </nav>

    <!-- Mobile Menu Toggle -->
    <button class="mobile-menu-toggle" onclick="toggleSidebar()">â˜°</button>

    <!-- Main Content -->
    <main class="main-content">
        <div class="page-header">
            <h1>Jupyter Notebook Complete Walkthrough</h1>
            <p>A comprehensive guide to the Credit_Risk_Dataset.ipynb notebook that trains our Machine Learning model</p>
        </div>

        <!-- Google Colab Link -->
        <div class="content-section" style="background: linear-gradient(135deg, rgba(80, 200, 120, 0.1) 0%, rgba(0, 31, 63, 0.05) 100%); border-left: 4px solid var(--emerald-green);">
            <h2 style="margin-top: 0;">ğŸ““ View on Google Colab</h2>
            <p>Access the complete notebook with interactive code cells and run it yourself!</p>
            <div class="text-center" style="margin-top: 1.5rem;">
                <a href="https://colab.research.google.com/drive/1PWf_V_HqnB-TavUPG-vRA3HtUj0xhuoE?usp=sharing" 
                   target="_blank" 
                   class="btn" 
                   style="text-decoration: none; background: linear-gradient(135deg, #f9ab00 0%, #ea4335 100%);">
                    ğŸš€ Open Notebook on Google Colab
                </a>
            </div>
        </div>

        <!-- Table of Contents -->
        <div class="content-section">
            <h2>ğŸ“š Table of Contents</h2>
            <ul style="columns: 2; column-gap: 2rem;">
                <li><a href="#overview" style="color: var(--navy-blue); text-decoration: none;">Overview</a></li>
                <li><a href="#section1" style="color: var(--navy-blue); text-decoration: none;">1. Import Libraries</a></li>
                <li><a href="#section2" style="color: var(--navy-blue); text-decoration: none;">2. Load Data & Feature Engineering</a></li>
                <li><a href="#section3" style="color: var(--navy-blue); text-decoration: none;">3. Data Cleaning & Sanity Checks</a></li>
                <li><a href="#section4" style="color: var(--navy-blue); text-decoration: none;">4. Data Splitting</a></li>
                <li><a href="#section5" style="color: var(--navy-blue); text-decoration: none;">5. Preprocessing Pipeline</a></li>
                <li><a href="#section6" style="color: var(--navy-blue); text-decoration: none;">6. Model Training</a></li>
                <li><a href="#section7" style="color: var(--navy-blue); text-decoration: none;">7. Loss Curve Visualization</a></li>
                <li><a href="#section8" style="color: var(--navy-blue); text-decoration: none;">8. Model Evaluation</a></li>
                <li><a href="#section9" style="color: var(--navy-blue); text-decoration: none;">9. Model Saving</a></li>
            </ul>
        </div>

        <!-- Overview -->
        <div class="content-section" id="overview">
            <h2>Overview</h2>
            <p>
                The <code>Credit_Risk_Dataset.ipynb</code> notebook is where we train our Random Forest Classifier model. 
                This notebook follows a complete Machine Learning workflow from data loading to model saving.
            </p>
            <div class="highlight-box">
                <h3>ğŸ“Š Dataset Information</h3>
                <ul>
                    <li><strong>Dataset:</strong> Credit Risk Dataset from Kaggle</li>
                    <li><strong>Total Records:</strong> 32,581 loan applications</li>
                    <li><strong>Features:</strong> 12 original features + 1 engineered feature (interest_burden)</li>
                    <li><strong>Target:</strong> loan_status (0 = Safe, 1 = Risky)</li>
                    <li><strong>Class Distribution:</strong> 78% Safe, 22% Risky (imbalanced dataset)</li>
                </ul>
            </div>
        </div>

        <!-- Section 1: Import Libraries -->
        <div class="content-section" id="section1">
            <h2>1ï¸âƒ£ Import Libraries</h2>
            <p>We start by importing all the necessary libraries for data processing, visualization, and machine learning.</p>
            
            <div class="code-block">
<code><span class="comment"># 1. Environment Setup | Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©</span>
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">import</span> seaborn <span class="keyword">as</span> sns
<span class="keyword">import</span> kagglehub
<span class="keyword">import</span> os

<span class="comment"># Machine Learning Libraries | Ù…ÙƒØªØ¨Ø§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©</span>
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, OneHotEncoder
<span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer
<span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline
<span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier
<span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report, ConfusionMatrixDisplay</code>
            </div>

            <h3>What Each Library Does:</h3>
            <div class="info-box">
                <ul>
                    <li><strong>pandas & numpy:</strong> Data manipulation and numerical operations</li>
                    <li><strong>matplotlib & seaborn:</strong> Data visualization and plotting</li>
                    <li><strong>kagglehub:</strong> Download datasets directly from Kaggle</li>
                    <li><strong>sklearn:</strong> Machine learning tools (preprocessing, models, evaluation)</li>
                </ul>
            </div>
        </div>

        <!-- Section 2: Load Data & Feature Engineering -->
        <div class="content-section" id="section2">
            <h2>2ï¸âƒ£ Load Data & Feature Engineering</h2>
            <p>We download the dataset from Kaggle and create a new feature called <code>interest_burden</code>.</p>

            <h3>Step 1: Download Dataset</h3>
            <div class="code-block">
<code><span class="comment"># Download dataset | ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª</span>
path = kagglehub.dataset_download(<span class="string">"laotse/credit-risk-dataset"</span>)
print(<span class="string">"Path:"</span>, path)</code>
            </div>
            <div class="info-box">
                <p><strong>What This Does:</strong> Downloads the Credit Risk Dataset from Kaggle using the kagglehub library. The dataset is automatically cached for faster access in future runs.</p>
            </div>

            <h3>Step 2: Load CSV File</h3>
            <div class="code-block">
<code><span class="comment"># Load the CSV file</span>
df = pd.read_csv(<span class="string">'credit_risk_dataset.csv'</span>)
print(f<span class="string">"Shape: {df.shape}"</span>)
df.head()</code>
            </div>
            <div class="info-box">
                <p><strong>Result:</strong> Dataset loaded with <strong>32,581 rows</strong> and <strong>12 columns</strong>.</p>
            </div>

            <h3>Step 3: Feature Engineering â­</h3>
            <div class="code-block">
<code><span class="comment"># --- Feature Engineering Step ---</span>
<span class="comment"># Create 'Interest Burden' = (Loan Amount * Interest Rate) / Income</span>
<span class="comment"># Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙŠØ²Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ù†Ø³Ø¨Ø© Ø¹Ø¨Ø¡ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ø®Ù„</span>
df[<span class="string">'interest_burden'</span>] = (df[<span class="string">'loan_amnt'</span>] * (df[<span class="string">'loan_int_rate'</span>] / <span class="string">100</span>)) / df[<span class="string">'person_income'</span>]
df[<span class="string">'interest_burden'</span>] = df[<span class="string">'interest_burden'</span>].fillna(<span class="string">0</span>) <span class="comment"># Handle potential division by zero</span>

print(f<span class="string">"Data Shape: {df.shape}"</span>)
print(<span class="string">"âœ… Feature Engineering Complete"</span>)</code>
            </div>

            <div class="highlight-box">
                <h3>ğŸ”‘ Why Feature Engineering?</h3>
                <p><strong>interest_burden</strong> is a calculated feature that shows what percentage of a person's income goes toward loan interest payments.</p>
                <p><strong>Formula:</strong> <code>interest_burden = (loan_amnt Ã— (loan_int_rate / 100)) / person_income</code></p>
                <p><strong>Why It Matters:</strong></p>
                <ul>
                    <li>A lower interest burden means the person can more easily afford loan payments</li>
                    <li>This single number captures an important financial relationship</li>
                    <li>Helps the model make better predictions about default risk</li>
                </ul>
                <p style="margin-top: 1rem;"><strong>Result:</strong> Dataset now has <strong>13 columns</strong> (12 original + 1 new feature).</p>
            </div>
        </div>

        <!-- Section 3: Data Cleaning -->
        <div class="content-section" id="section3">
            <h2>3ï¸âƒ£ Data Cleaning & Sanity Checks</h2>
            <p>We remove outliers and check data quality to ensure our model trains on clean, realistic data.</p>

            <div class="code-block">
<code><span class="comment"># 3. Sanity Checks (Cleaning) | Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ</span>
<span class="comment"># Remove impossible ages (Outliers)</span>
<span class="comment"># Ø­Ø°Ù Ø§Ù„Ø£Ø¹Ù…Ø§Ø± Ø§Ù„Ù…Ø³ØªØ­ÙŠÙ„Ø© (Ø£ÙƒØ¨Ø± Ù…Ù† 100 Ø³Ù†Ø©)</span>
df_clean = df[df[<span class="string">'person_age'</span>] <= <span class="string">100</span>].copy()

<span class="comment"># Check Imbalance</span>
<span class="comment"># Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØ§Ø²Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª</span>
print(<span class="string">"\nTarget Distribution (Loan Status):"</span>)
print(df_clean[<span class="string">'loan_status'</span>].value_counts(normalize=<span class="string">True</span>))</code>
            </div>

            <div class="info-box">
                <h3>What This Does:</h3>
                <ul>
                    <li><strong>Removes Outliers:</strong> Filters out records where age > 100 (impossible values)</li>
                    <li><strong>Checks Class Distribution:</strong> Shows the proportion of Safe (0) vs Risky (1) loans</li>
                </ul>
                <p style="margin-top: 1rem;"><strong>Class Distribution Result:</strong></p>
                <ul>
                    <li><strong>Safe (0):</strong> 78.18%</li>
                    <li><strong>Risky (1):</strong> 21.82%</li>
                </ul>
                <p style="margin-top: 1rem; color: var(--navy-blue); font-weight: 600;">
                    âš ï¸ This is an <strong>imbalanced dataset</strong> - we'll use stratified splitting to handle this!
                </p>
            </div>

            <h3>Data Exploration</h3>
            <p>The notebook also includes:</p>
            <ul>
                <li><code>df.info()</code>: Shows data types and missing values</li>
                <li><code>df.describe()</code>: Shows statistical summary (mean, min, max, etc.)</li>
                <li><code>df.head()</code>: Displays first few rows</li>
            </ul>
        </div>

        <!-- Section 4: Data Splitting -->
        <div class="content-section" id="section4">
            <h2>4ï¸âƒ£ Data Splitting</h2>
            <p>We split the data into training and testing sets using <strong>stratified splitting</strong> to handle class imbalance.</p>

            <div class="code-block">
<code><span class="comment"># 4. Stratified Splitting | Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø·Ø¨Ù‚ÙŠ</span>
X = df_clean.drop(<span class="string">'loan_status'</span>, axis=<span class="string">1</span>)
y = df_clean[<span class="string">'loan_status'</span>]

<span class="comment"># Split 80% Train, 20% Test with Stratify</span>
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=<span class="string">0.2</span>, random_state=<span class="string">42</span>, stratify=y
)

print(f<span class="string">"\nTraining Set: {X_train.shape}"</span>)
print(f<span class="string">"Test Set: {X_test.shape}"</span>)</code>
            </div>

            <div class="highlight-box">
                <h3>ğŸ’¡ Why Stratified Splitting?</h3>
                <p><strong>stratify=y</strong> ensures that both training and test sets have the same proportion of Safe vs Risky loans as the original dataset.</p>
                <p><strong>Why This Matters:</strong></p>
                <ul>
                    <li>Prevents the test set from having too many or too few risky loans</li>
                    <li>Ensures fair evaluation of the model</li>
                    <li>Critical for imbalanced datasets like ours (78% safe, 22% risky)</li>
                </ul>
                <p style="margin-top: 1rem;"><strong>Result:</strong></p>
                <ul>
                    <li><strong>Training Set:</strong> 26,060 samples (80%)</li>
                    <li><strong>Test Set:</strong> 6,516 samples (20%)</li>
                </ul>
            </div>
        </div>

        <!-- Section 5: Preprocessing Pipeline -->
        <div class="content-section" id="section5">
            <h2>5ï¸âƒ£ Preprocessing Pipeline</h2>
            <p>We build a Pipeline to automatically handle missing values, encode categorical data, and scale numerical features.</p>

            <div class="code-block">
<code><span class="comment"># 5. Preprocessing Pipeline | Ø®Ø· Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©</span>
<span class="comment"># Define transformers for numeric and categorical columns</span>

numeric_transformer = Pipeline(steps=[
    (<span class="string">'imputer'</span>, SimpleImputer(strategy=<span class="string">'mean'</span>)),
    (<span class="string">'scaler'</span>, StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    (<span class="string">'imputer'</span>, SimpleImputer(strategy=<span class="string">'most_frequent'</span>)),
    (<span class="string">'encoder'</span>, OneHotEncoder(drop=<span class="string">'first'</span>, sparse_output=<span class="string">False</span>))
])

<span class="comment"># Combine transformers</span>
preprocessor = ColumnTransformer(
    transformers=[
        (<span class="string">'num'</span>, numeric_transformer, numeric_features),
        (<span class="string">'cat'</span>, categorical_transformer, categorical_features)
    ]
)</code>
            </div>

            <h3>Pipeline Steps Explained:</h3>
            <div class="info-box">
                <h4>For Numerical Features:</h4>
                <ol>
                    <li><strong>SimpleImputer:</strong> Fills missing values with the mean (average) of that column</li>
                    <li><strong>StandardScaler:</strong> Normalizes values to have mean=0 and std=1 (puts everything on the same scale)</li>
                </ol>
            </div>

            <div class="info-box">
                <h4>For Categorical Features:</h4>
                <ol>
                    <li><strong>SimpleImputer:</strong> Fills missing values with the most frequent (mode) value</li>
                    <li><strong>OneHotEncoder:</strong> Converts text categories (like "RENT", "OWN") into numbers (0s and 1s)</li>
                </ol>
            </div>

            <div class="highlight-box">
                <h3>ğŸ”§ Why Use a Pipeline?</h3>
                <ul>
                    <li><strong>Consistency:</strong> Same preprocessing applied during training AND prediction</li>
                    <li><strong>Organization:</strong> All preprocessing steps in one place</li>
                    <li><strong>Easier Deployment:</strong> Save the entire pipeline, not just the model</li>
                    <li><strong>Prevents Bugs:</strong> Can't forget a preprocessing step!</li>
                </ul>
            </div>
        </div>

        <!-- Section 6: Model Training -->
        <div class="content-section" id="section6">
            <h2>6ï¸âƒ£ Model Training</h2>
            <p>We combine the preprocessing pipeline with the Random Forest Classifier and train the model.</p>

            <div class="code-block">
<code><span class="comment"># 6. Model Training | ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬</span>
<span class="comment"># Define the full pipeline with the model</span>
model = Pipeline(steps=[
    (<span class="string">'preprocessor'</span>, preprocessor),
    (<span class="string">'classifier'</span>, RandomForestClassifier(
        n_estimators=<span class="string">100</span>,
        random_state=<span class="string">42</span>,
        n_jobs=-<span class="string">1</span>
    ))
])

print(<span class="string">"\nğŸš€ Training Model..."</span>)
model.fit(X_train, y_train)
print(<span class="string">"âœ… Training Complete!"</span>)</code>
            </div>

            <div class="highlight-box">
                <h3>ğŸ¯ Random Forest Parameters:</h3>
                <ul>
                    <li><strong>n_estimators=100:</strong> Creates 100 decision trees (more trees = better accuracy, but slower)</li>
                    <li><strong>random_state=42:</strong> Ensures reproducible results (same random seed every time)</li>
                    <li><strong>n_jobs=-1:</strong> Uses all CPU cores for faster training</li>
                </ul>
            </div>

            <div class="info-box">
                <h3>What Happens During Training:</h3>
                <ol>
                    <li>The preprocessor transforms the training data (impute, scale, encode)</li>
                    <li>Random Forest builds 100 decision trees</li>
                    <li>Each tree learns patterns from the data</li>
                    <li>The trees vote together to make predictions</li>
                </ol>
                <p style="margin-top: 1rem;"><strong>Training Time:</strong> Usually takes a few minutes depending on your computer's speed.</p>
            </div>
        </div>

        <!-- Section 7: Loss Curve Visualization -->
        <div class="content-section" id="section7">
            <h2>7ï¸âƒ£ Loss Curve Visualization</h2>
            <p>We visualize the training process using a Neural Network to ensure the model is learning properly.</p>

            <div class="code-block">
<code><span class="comment"># We use a separate MLP model just to visualize the loss curve</span>
diag_model = MLPClassifier(hidden_layer_sizes=(<span class="string">32</span>,), max_iter=<span class="string">50</span>, random_state=<span class="string">42</span>)
diag_model.fit(X_train_transformed, y_train)

<span class="comment"># Plot the loss curve</span>
plt.plot(diag_model.loss_curve_, label=<span class="string">'Training Loss'</span>, color=<span class="string">'blue'</span>)
plt.xlabel(<span class="string">'Iteration'</span>)
plt.ylabel(<span class="string">'Loss'</span>)
plt.title(<span class="string">'Training Loss Curve'</span>)
plt.legend()
plt.show()</code>
            </div>

            <div class="info-box">
                <h3>Why Visualize Loss?</h3>
                <ul>
                    <li><strong>Confirms Learning:</strong> If loss decreases over time, the model is learning</li>
                    <li><strong>Detects Overfitting:</strong> If loss stops decreasing, we might need to stop training</li>
                    <li><strong>Diagnostic Tool:</strong> Helps identify training problems early</li>
                </ul>
                <p style="margin-top: 1rem;"><strong>Note:</strong> Random Forest doesn't have a loss curve (it's not iterative), so we use MLPClassifier just for visualization purposes.</p>
            </div>
        </div>

        <!-- Section 8: Model Evaluation -->
        <div class="content-section" id="section8">
            <h2>8ï¸âƒ£ Model Evaluation</h2>
            <p>We test the trained model on unseen test data to measure its performance.</p>

            <div class="code-block">
<code><span class="comment"># 8. Evaluation | Ø§Ù„ØªÙ‚ÙŠÙŠÙ…</span>
y_pred = model.predict(X_test)

<span class="comment"># Classification Report</span>
print(classification_report(y_test, y_pred))

<span class="comment"># Confusion Matrix</span>
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
plt.show()</code>
            </div>

            <div class="highlight-box">
                <h3>ğŸ“Š Evaluation Metrics:</h3>
                <ul>
                    <li><strong>Accuracy:</strong> Overall percentage of correct predictions</li>
                    <li><strong>Precision:</strong> Of all predicted risky loans, how many were actually risky?</li>
                    <li><strong>Recall:</strong> Of all actual risky loans, how many did we catch?</li>
                    <li><strong>F1-Score:</strong> Balance between precision and recall</li>
                </ul>
            </div>

            <div class="info-box">
                <h3>Confusion Matrix:</h3>
                <p>A table showing:</p>
                <ul>
                    <li><strong>True Positives:</strong> Correctly predicted risky loans</li>
                    <li><strong>True Negatives:</strong> Correctly predicted safe loans</li>
                    <li><strong>False Positives:</strong> Predicted risky but actually safe (Type I error)</li>
                    <li><strong>False Negatives:</strong> Predicted safe but actually risky (Type II error)</li>
                </ul>
                <p style="margin-top: 1rem;"><strong>Goal:</strong> Maximize True Positives and True Negatives, minimize False Positives and False Negatives.</p>
            </div>
        </div>

        <!-- Section 9: Model Saving -->
        <div class="content-section" id="section9">
            <h2>9ï¸âƒ£ Model Saving</h2>
            <p>Finally, we save the trained model so we can use it in our Flask web application.</p>

            <div class="code-block">
<code><span class="keyword">import</span> joblib

<span class="comment"># Save the complete pipeline (preprocessor + model)</span>
joblib.dump(model, <span class="string">'credit_risk_model.pkl'</span>)

print(<span class="string">"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„! Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù…Ù„Ù credit_risk_model.pkl Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©"</span>)
print(<span class="string">"Model saved! Download credit_risk_model.pkl from the sidebar"</span>)</code>
            </div>

            <div class="highlight-box">
                <h3>ğŸ’¾ Why Save the Complete Pipeline?</h3>
                <p>We save the <strong>entire pipeline</strong> (preprocessor + classifier), not just the model, because:</p>
                <ul>
                    <li>The preprocessor is needed for every prediction</li>
                    <li>Ensures consistent preprocessing (same as training)</li>
                    <li>Simplifies deployment - just load and use!</li>
                </ul>
                <p style="margin-top: 1rem;"><strong>File Size:</strong> The saved model is typically 20-50 MB (before compression).</p>
            </div>

            <div class="info-box">
                <h3>Next Steps:</h3>
                <ol>
                    <li>Download the <code>credit_risk_model.pkl</code> file from Google Colab</li>
                    <li>Place it in the same folder as your <code>app.py</code> file</li>
                    <li>Use <code>joblib.load()</code> in your Flask app to load the model</li>
                    <li>Start making predictions!</li>
                </ol>
            </div>
        </div>

        <!-- Complete Workflow Summary -->
        <div class="content-section">
            <h2>ğŸ”„ Complete Workflow Summary</h2>
            <div class="highlight-box">
                <ol style="line-height: 2;">
                    <li><strong>Import Libraries</strong> â†’ Load all necessary tools</li>
                    <li><strong>Load Data</strong> â†’ Download dataset from Kaggle</li>
                    <li><strong>Feature Engineering</strong> â†’ Create <code>interest_burden</code> feature</li>
                    <li><strong>Data Cleaning</strong> â†’ Remove outliers and check quality</li>
                    <li><strong>Data Splitting</strong> â†’ Split into train/test with stratification</li>
                    <li><strong>Build Pipeline</strong> â†’ Create preprocessing pipeline</li>
                    <li><strong>Train Model</strong> â†’ Fit Random Forest on training data</li>
                    <li><strong>Visualize</strong> â†’ Plot loss curve (diagnostic)</li>
                    <li><strong>Evaluate</strong> â†’ Test on unseen data</li>
                    <li><strong>Save Model</strong> â†’ Export for use in Flask app</li>
                </ol>
            </div>
        </div>

        <!-- Key Takeaways -->
        <div class="content-section">
            <h2>ğŸ¯ Key Takeaways</h2>
            <div class="info-box">
                <ul style="line-height: 2;">
                    <li><strong>Feature Engineering:</strong> Creating <code>interest_burden</code> improves model accuracy</li>
                    <li><strong>Stratified Splitting:</strong> Essential for imbalanced datasets</li>
                    <li><strong>Pipeline:</strong> Ensures consistent preprocessing during training and prediction</li>
                    <li><strong>Random Forest:</strong> Powerful algorithm that works well on structured financial data</li>
                    <li><strong>Evaluation:</strong> Always test on unseen data to measure real performance</li>
                    <li><strong>Model Saving:</strong> Save the complete pipeline, not just the classifier</li>
                </ul>
            </div>
        </div>

        <!-- Google Colab Link Again -->
        <div class="content-section text-center" style="background: linear-gradient(135deg, rgba(80, 200, 120, 0.1) 0%, rgba(0, 31, 63, 0.05) 100%);">
            <h2>Ready to Explore?</h2>
            <p>Open the notebook on Google Colab to see the code in action and run it yourself!</p>
            <a href="https://colab.research.google.com/drive/1PWf_V_HqnB-TavUPG-vRA3HtUj0xhuoE?usp=sharing" 
               target="_blank" 
               class="btn" 
               style="text-decoration: none; background: linear-gradient(135deg, #f9ab00 0%, #ea4335 100%); margin-top: 1rem;">
                ğŸš€ Open Notebook on Google Colab
            </a>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="footer-content">
            <h3>Connect With Me</h3>
            <p>Follow me on social media for more projects and updates</p>
            <div class="social-links">
                <a href="https://github.com/Tegnh" target="_blank" class="social-link github" title="GitHub">
                    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                        <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                    </svg>
                </a>
                <a href="https://x.com/6ar_t" target="_blank" class="social-link twitter" title="Twitter/X">
                    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                        <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
                    </svg>
                </a>
                <a href="https://www.linkedin.com/in/tarig-fdl-7a6599348/" target="_blank" class="social-link linkedin" title="LinkedIn">
                    <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                </a>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2026 Smart Credit Risk System. Built with â¤ï¸ by Tegnh</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
